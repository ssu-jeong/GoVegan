### 쇼핑몰 상품 크롤링

---

##### params 및 header 설정

- params와 header는 딕셔너리 구조로 설정.

   - params에는 받아올 값을 입력하고 header에는 user-agent나 cookie같은 것을 입력

- user-agent는 F12(검사)에서 Network탭을 누른 후, Doc탭을 눌러서 확인

- user-agent가 없으면 크롤링이 안될 수 있으니 가급적 설정


##### 페이지 소스 확인

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcS1vqw%2Fbtq3PQOWBb9%2FiqboUCoiIY0R1IgcnKD7E0%2Fimg.png)

- 상품 리스트: ul태그에 id:productList

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FXz6j9%2Fbtq3TKGRWVn%2F2VssxMNTRPNk2v4en1Hhw0%2Fimg.png)

- 상품 하나: ul태그 아래에 li태그, class: baby-product renew-badge

   - 아이템은 여러개 이므로, id가 아니라 class로 걸러내야함

##### 소스코드

```python
import requests
from bs4 import BeautifulSoup

headers = {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}

def getPageString(url):
    data = requests.get(url, headers = headers)
    print(data.content)
    return data.content

def getProducts(string):
    bsObj = BeautifulSoup(string, "html.parser")
    ul = bsObj.find("ul", {"id":"productList"})  #아이템 리스트부분 추출
    lis = ul.findAll("li", {"class":"baby-product renew-badge"}) #각 아이템 추출

    for item in lis:
        #url
        a = item.find("a", {"class": "baby-product-link"})
        url = a.get('href')
        print("url:", url)

        #name
        div_name = item.find("div", {"class":"name"})
        name = div_name.getText()
        print("name:", name)

        #image
        dt_image = item.find("dt", {"class":"image"})
        image = dt_image.find("img").get('src')
        print("image:", image)

        #price
        price = item.find("strong", {"class":"price-value"}).getText()
        print("price:", price)

    print(len(lis))
    return []

url = "https://www.coupang.com/np/categories/465075"

pageString = getPageString(url)
print(getProducts(pageString))


from bs4 import BeautifulSoup

bsObj = BeautifulSoup(string, "html.parser") #string 부분은 가져온 html코드

#1개만 있는 경우 find 사용, 앞에 태그, (선택사항)뒤엔 중괄호로 id, class
ul = bsObj.find("ul", {"id":"value"})  
a = bsObj.find("a", {"class": "value"})

#여러개인 경우 findAll 또는 find_all, 배열로 생성됨
lis = ul.findAll("li", {"class":"value"})

#추출한 태그에서 속성 또는 값 가져올 때
href = a.get('href')  #href속성 가져옴
text = a.geText()  #태그 사이 글씨 가져옴 ex) <h1>글씨</h1>
```

- 먼저 아이템리스트에 해당하는 ul부분 추출

- 각 아이템 추출, lis는 배열

- lis요소 1개씩 돌아가면서 url,name,image,price 추출

- lis길이를 출력하여 모든 아이템을 불러왔는지 확인

##### 소스코드 - 검색결과크롤링

```python

#검색 키워드 입력받는 keyword설정
keyword = input('검색키워드: ')

params = {'q' : keyword,}

#user-agent header설정
headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36'}


#검색 목록 크롤링
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib import parse
from datetime import datetime

result_list = []

res = requests.get(url, params=params, headers=headers)

if res.status_code == 200:
    soup = BeautifulSoup(res.text)
    item_list = soup.select('ul#productList li')
    base_url = 'https://www.cupang.com/'
    error_cnt = 0
    for item in item_list:
        try:
            item_name = item.select_one('div.name').text.strip()
            link = item.select_one('a').get('href')
            link = parse.urljoin(base_url, link)
            price = item.select_one('strong.price-value').text.strip()
            price = ''.join(price.spilt(','))
            result_list.append([item_name, link, price])

        except Exception as e:
            print(e)
            error_cnt +=1

#크롤링 내역 저장
data = datetime.now().strftime('%Y-%m-%d')
filename = f'쿠팡_{keyword}_조회결과_{date}'
df = pd.DataFrame(result_list, columns = ['title', 'link', 'price'])
df.to_csv(filename+".csv", index=False, encoding='UTF-8')

#크롤링을 위한 기본 설정 및 마지막 페이지 가지고오기
base_url = 'https://www.coupang.com/np/search?q={}&page={}'
keyword = input("검색할 키워드: ")
headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36'}

result_list = []
url = base_url.fomat(keyword,1)
print(url)

res = requests.get(url, headers=headers)
if res.status_code == 200:
    soup = BeatifulSoup(res.text)
    print(soup)
    last_page = soup.select_one('a.btn-last').text.strip()
    print(last_page)

#각페이지의 상품조회
error_cnt = 0
cp_url = 'https://www.coupang.com/'
for page in range(1, int(last_page)+1):
    url = base_url.fomat(keyword, page)
    res = requests.get(url, headers=headers)
    if res.status_code == 200:
        soup = BeautifulSoup(res.text)
        item_list = soup.select('ul#productList li')
        for item in item_list:
            try:
                item_name = item.select_one('div.name').text.strip()
                link = item.select_one('a').get('href')
                link = parse.urljoin(base_url, link)
                price = item.select_one('strong.price-value').text.strip()
                price = ''.join(price.spilt(','))
                result_list.append([item_name, link, price])

            except Exception as e:
                print(e)
                error_cnt +=1


```

#### Reference

---

- [쿠팡 크롤링](https://suyeoniii.tistory.com/40)

- [쿠팡검색결과 크롤링](https://ysyblog.tistory.com/52)

- [네이버쇼핑 크롤링](https://davincii.tistory.com/774?category=344085)


