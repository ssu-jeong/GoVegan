{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"프로젝트6.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOCM97ikKeJW4P6iS3bC6FZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vA4a9plZHI9p"},"outputs":[],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","#드라이브에 접근할 수 있도록 아래 코드 입력\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#불러올 파일의 경로를 filename 변수에 저장\n","filename = '/content/drive/MyDrive/AI부트캠프/프로젝트/프로젝트6/쿠팡조회결과.csv'\n","\n","#pandas read_csv로 불러오기\n","df = pd.read_csv(filename)\n","df.head()"],"metadata":{"id":"_v95nv9VI8L8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install konlpy"],"metadata":{"id":"GvHWwiUoJVuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Okt, Kkma, Hannanum\n","import re\n","\n","df_name = df['name']\n","df_name"],"metadata":{"id":"tAxuyxJpJTCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#전처리\n","\n","# 정규식\n","# []: [] 사이 문자를 매치, ^: not\n","regex = r\"[^가-힣]\"\n","\n","\n","# 치환할 문자\n","subst = \"\"\n","\n","\n","def clean_text(texts):\n","    corpus = [] \n","    for i in range(0, len(texts)): \n","        text = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation \n","        text = re.sub(r'\\d+','', str(texts[i]))# remove number \n","        text = text.lower() #lower case \n","        text = re.sub(r'\\s+', ' ', text) #remove extra space \n","        text = re.sub(r'<[^>]+>','',text) #remove Html tags \n","        text = re.sub(r'\\s+', ' ', text) #remove spaces \n","        text = re.sub(r\"^\\s+\", '', text) #remove space from start \n","        text = re.sub(r'\\s+$', '', text) #remove space from the end\n","        text = re.compile(regex).sub(subst, text) #한글만\n","        corpus.append(text) \n","    return corpus\n","\n","df['clean_name'] = clean_text(df_name)"],"metadata":{"id":"VGPZQbkLqmhI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(clean_text(df_name))"],"metadata":{"id":"YKFRi9WbrRhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = '프로게이너아몬드프로틴비건인증식물성프로틴비건식단채식식단단백질쉐이크'\n","\n","def tokenize(text):\n","    \"\"\"text 문자열을 의미있는 단어 단위로 list에 저장합니다.\n","    Args:\n","        text (str): 토큰화 할 문자열\n","    Returns:\n","        list: 토큰이 저장된 리스트\n","    \"\"\"\n","    okt = Okt() \n","\n","    tokens = okt.morphs(text) #morphs: 형태소 단위로 구문을 분석\n","    \n","    return tokens\n","\n","print(tokenize(test))"],"metadata":{"id":"0RyLDRJHajrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Han = Hannanum()\n","\n","print(Han.analyze(test))"],"metadata":{"id":"GpNQSv23n2sI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tokens'] = df['clean_name'].apply(tokenize)\n","df['tokens'].head()"],"metadata":{"id":"8J8bgunha1jq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Kkma = Kkma()\n","\n","print(Kkma.morphs(test))"],"metadata":{"id":"sL4mv7uJ5tZt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#불용어 처리\n","stopwords = ['개', '개입', '입'] \n","def remove_stopword_text(text): \n","    corpus = [] \n","    for sent in text: \n","        modi_sent = [] \n","        for word in sent: \n","            if word not in stopwords: \n","                modi_sent.append(word) \n","        corpus.append(''.join(modi_sent)) \n","    return corpus\n","\n","df['tokens'] = df['tokens'].apply(remove_stopword_text)\n","df['tokens'].head()"],"metadata":{"id":"VETTEq71ZEV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tokens'] = df['tokens'].apply(lambda x : (' ').join(x))\n","\n","df['tokens']"],"metadata":{"id":"s4FPirBvd-re"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"MfVxnhebLLgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_name = df_name.apply(remove_stopword_text)\n","df_name"],"metadata":{"id":"RBitL5kowrPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['clean_name'] = df['clean_name'].apply(lambda x : (' ').join(x))\n","df['clean_name']"],"metadata":{"id":"Bx_x63Q3w5u_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # replace 함수를 이용해서 공백 제거 \n","# df['clean_name'] = df['clean_name'].replace(\" \" , \"\")"],"metadata":{"id":"6ASmAe1g00PE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"8JqLHbF91A8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coupangdata = df.loc[:, [col for col in df.columns if col != 'clean_name']]"],"metadata":{"id":"icwgOoBUPeu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coupangdata"],"metadata":{"id":"4H1hp8HlPlP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coupangdata.to_csv('coupangdata.csv', index=False)"],"metadata":{"id":"hDmz6xRU28A5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokens에 대해서 tf-idf 수행\n","\n","tfidf = TfidfVectorizer()\n","tfidf_matrix = tfidf.fit_transform(df['tokens'])\n","print(tfidf_matrix.shape)"],"metadata":{"id":"8J1Xz9d6V1GY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"],"metadata":{"id":"hIXlaHdRWBDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(cosine_sim)"],"metadata":{"id":"FJFtV0gGJ0as"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["indices = pd.Series(df.index, index=df['tokens']).drop_duplicates()\n","print(indices.head())"],"metadata":{"id":"oGUd343sWCR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = indices['프로 게 이 너 아몬드 프로 틴 비건 인증 식물성 프로 틴 비건 식단 채식 식단 단백질 쉐이크']\n","print(idx)"],"metadata":{"id":"r0vTbR0FWLhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sim_scores = list(enumerate(cosine_sim[4]))"],"metadata":{"id":"ikWKctLwWxK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sim_scores)"],"metadata":{"id":"zkOKVzmzJ-zy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)"],"metadata":{"id":"y5t-tqmOW_LY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sim_scores)"],"metadata":{"id":"VaGdtvKTKCg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sim_scores = sim_scores[1:4]"],"metadata":{"id":"snmevjr1XFws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sim_scores)"],"metadata":{"id":"no70HFY3KLLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["name_indices = [i[0] for i in sim_scores]"],"metadata":{"id":"0yJ5KJHBXHqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_recommendations(name, cosine_sim=cosine_sim):\n","    # 선택한 상품의 이름으로부터 해당되는 인덱스를 받아온다.\n","    idx = indices[name]\n","\n","    # 모든 상품에 대해서 해당 상품와의 유사도를 구한다.\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","\n","    # 유사도에 따라 상품들을 정렬한다.\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","\n","    # 가장 유사한 3개의 상품이름을 받아온다.\n","    sim_scores = sim_scores[1:6]\n","\n","    # 가장 유사한 3개의 상품이름의 인덱스를 얻는다.\n","    name_indices = [i[0] for i in sim_scores]\n","\n","    # 가장 유사한 3개의 상품 이름을 리턴한다.\n","    return df['tokens'].iloc[name_indices]"],"metadata":{"id":"O_4vw5n5WRRe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#입력데이터 값이 파일에 있는 것과 정확히 동일해야만 작동\n","get_recommendations('프로 게 이 너 아몬드 프로 틴 비건 인증 식물성 프로 틴 비건 식단 채식 식단 단백질 쉐이크')"],"metadata":{"id":"MfiM83pzWe6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","# Counter 객체는 리스트요소의 값과 요소의 갯수를 카운트 하여 저장하고 있습니다.\n","# 카운터 객체는 .update 메소드로 계속 업데이트 가능합니다.\n","word_counts = Counter()\n","\n","# 토큰화된 각 리뷰 리스트를 카운터 객체에 업데이트 합니다. \n","df['tokens'].apply(lambda x: word_counts.update(x))\n","\n","# 가장 많이 존재하는 단어 순으로 10개를 나열합니다\n","word_counts.most_common(20)"],"metadata":{"id":"4R70nosxiB4f"},"execution_count":null,"outputs":[]}]}